# Chinese BERT Models Fine-Tuning for Stereotype Detection
# ä¸­æ–‡åˆ»æ¿å°è±¡æ£€æµ‹æ¨¡å‹å¾®è°ƒ
# åŸºäº HEARTS æ¡†æ¶å¤ç°

# ==================== [FIX 0] ä¾èµ–ç‰ˆæœ¬ä¿®å¤ ====================
# transformers v5.1.0 è¦æ±‚ accelerate >= 1.5.0ï¼Œå¦åˆ™ Trainer ä¼šå´©æºƒ
# åœ¨ Colab é¡¶éƒ¨ cell è¿è¡Œ:
#   !pip install --upgrade accelerate>=1.5.0
# æˆ–å–æ¶ˆä¸‹é¢ä¸¤è¡Œçš„æ³¨é‡Šåœ¨è„šæœ¬å†…è‡ªåŠ¨å‡çº§:
# import subprocess, sys
# subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade", "accelerate>=1.5.0"])

import pandas as pd
import numpy as np
import os
import logging
from pathlib import Path
from collections import OrderedDict
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, precision_recall_fscore_support, balanced_accuracy_score
from datasets import Dataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, pipeline
from codecarbon import EmissionsTracker
import torch
import transformers

# ==================== ç¯å¢ƒé…ç½® ====================
# è®¾ç½® Hugging Face é•œåƒåœ°å€ï¼ˆå›½å†…åŠ é€Ÿï¼‰
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ["HUGGINGFACE_TRAINER_ENABLE_PROGRESS_BAR"] = "1"

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.INFO)

# åŸºç¡€ç›®å½•é…ç½®
BASE_DIR = Path.cwd()

# ==================== ä¸­æ–‡æ¨¡å‹é…ç½® ====================
# å®šä¹‰ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆå¯¹æ ‡åŸè®ºæ–‡çš„è‹±æ–‡æ¨¡å‹ï¼‰
CHINESE_MODELS = {
    'bert': 'bert-base-chinese',                          # å¯¹åº” BERT-base-uncased
    'albert': 'uer/albert-base-chinese-cluecorpussmall',  # å¯¹åº” ALBERT-V2
    'rbt6': 'hfl/rbt6',                                   # å¯¹åº” DistilBERT (6å±‚RoBERTa)
    'macbert': 'hfl/chinese-macbert-base',                # é¢å¤–å¯¹æ¯”ï¼šMacBERT
}

# ==================== æ•°æ®åŠ è½½å‡½æ•° ====================
def data_loader(csv_file_path, labelling_criteria, dataset_name, sample_size=1000000, num_examples=5, test_size=0.2, random_state=42):
    """
    åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†
    
    Args:
        csv_file_path: CSVæ–‡ä»¶è·¯å¾„
        labelling_criteria: æ­£ç±»æ ‡ç­¾ï¼ˆå°†è¢«æ˜ å°„ä¸º1ï¼‰
        dataset_name: æ•°æ®é›†åç§°
        sample_size: é‡‡æ ·å¤§å°ï¼ˆå¦‚æœæ•°æ®é‡è¶…è¿‡æ­¤å€¼åˆ™é‡‡æ ·ï¼‰
        num_examples: æ‰“å°çš„ç¤ºä¾‹æ•°é‡
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        random_state: éšæœºç§å­
    
    Returns:
        train_data, test_data: è®­ç»ƒé›†å’Œæµ‹è¯•é›†DataFrame
    """
    print(f"\n{'='*60}")
    print(f"Loading dataset: {dataset_name}")
    print(f"{'='*60}")
    
    # è¯»å–æ•°æ®
    combined_data = pd.read_csv(csv_file_path, usecols=['text', 'label', 'group'])
    print(f"Original data size: {len(combined_data)}")
    print(f"Label distribution:\n{combined_data['label'].value_counts()}")
    
    # æ ‡ç­¾äºŒå€¼åŒ–
    label2id = {label: (1 if label == labelling_criteria else 0) for label in combined_data['label'].unique()}
    combined_data['label'] = combined_data['label'].map(label2id)
    print(f"Label mapping: {label2id}")
    
    # æ·»åŠ æ•°æ®é›†åç§°æ ‡è¯†
    combined_data['data_name'] = dataset_name
    
    # é‡‡æ ·ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if sample_size < len(combined_data):
        sample_proportion = sample_size / len(combined_data)
        sampled_data, _ = train_test_split(
            combined_data, 
            train_size=sample_proportion, 
            stratify=combined_data['label'],
            random_state=random_state
        )
        print(f"Sampled data size: {len(sampled_data)}")
    else:
        sampled_data = combined_data
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_data, test_data = train_test_split(
        sampled_data, 
        test_size=test_size, 
        random_state=random_state,
        stratify=sampled_data['label']
    )
    
    print(f"\nTrain data size: {len(train_data)}")
    print(f"Test data size: {len(test_data)}")
    print(f"Train label distribution:\n{train_data['label'].value_counts()}")
    print(f"Test label distribution:\n{test_data['label'].value_counts()}")
    
    print(f"\nFirst {num_examples} examples from training data:")
    print(train_data.head(num_examples))
    
    return train_data, test_data


# ==================== æ¨¡å‹è®­ç»ƒå‡½æ•° ====================
def train_model(train_data, model_path, model_name, batch_size=32, epochs=6, learning_rate=2e-5, 
                model_output_base_dir='model_output', dataset_name='cold', seed=42):
    """
    å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
    
    Args:
        train_data: è®­ç»ƒæ•°æ®DataFrame
        model_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºè¾“å‡ºç›®å½•å‘½åï¼‰
        batch_size: æ‰¹æ¬¡å¤§å°
        epochs: è®­ç»ƒè½®æ•°
        learning_rate: å­¦ä¹ ç‡
        model_output_base_dir: æ¨¡å‹è¾“å‡ºåŸºç¡€ç›®å½•
        dataset_name: æ•°æ®é›†åç§°
        seed: éšæœºç§å­
    
    Returns:
        model_output_dir: æ¨¡å‹ä¿å­˜è·¯å¾„
        training_emissions: è®­ç»ƒé˜¶æ®µç¢³æ’æ”¾é‡
    """
    print(f"\n{'='*60}")
    print(f"Training model: {model_name}")
    print(f"Model path: {model_path}")
    print(f"{'='*60}")
    
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    num_labels = len(train_data['label'].unique())
    print(f"Number of unique labels: {num_labels}")
    
    # å¯åŠ¨ç¢³æ’æ”¾è¿½è¸ª
    tracker = EmissionsTracker()
    tracker.start()
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    print("Loading model and tokenizer...")
    model = AutoModelForSequenceClassification.from_pretrained(
        model_path, 
        num_labels=num_labels, 
        ignore_mismatched_sizes=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # åˆ†è¯å‡½æ•°
    def tokenize_function(examples):
        return tokenizer(
            examples["text"], 
            padding=True, 
            truncation=True, 
            max_length=512
        )
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_split, val_split = train_test_split(
        train_data, 
        test_size=0.2, 
        random_state=seed,
        stratify=train_data['label']
    )
    print(f"Training split size: {len(train_split)}")
    print(f"Validation split size: {len(val_split)}")
    
    # [FIX 1] reset_index é¿å… __index_level_0__ æ®‹ç•™åˆ—
    tokenized_train = Dataset.from_pandas(train_split.reset_index(drop=True)).map(
        tokenize_function, batched=True
    )
    tokenized_train = tokenized_train.rename_column('label', 'labels')  # [FIX 2] ç”¨ rename_column æ›¿ä»£ map

    tokenized_val = Dataset.from_pandas(val_split.reset_index(drop=True)).map(
        tokenize_function, batched=True
    )
    tokenized_val = tokenized_val.rename_column('label', 'labels')  # [FIX 2]

    # [FIX 3] ç§»é™¤ä¸éœ€è¦çš„åˆ—ï¼Œé¿å… Trainer è­¦å‘Š
    cols_to_remove = [c for c in tokenized_train.column_names if c not in ['input_ids', 'attention_mask', 'token_type_ids', 'labels']]
    tokenized_train = tokenized_train.remove_columns(cols_to_remove)
    tokenized_val = tokenized_val.remove_columns(cols_to_remove)
    
    print(f"Sample tokenized input: {tokenized_train[0]}")
    
    # è¯„ä¼°æŒ‡æ ‡
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')
        balanced_acc = balanced_accuracy_score(labels, predictions)
        return {
            "precision": precision, 
            "recall": recall, 
            "f1": f1, 
            "balanced_accuracy": balanced_acc
        }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    model_output_dir = os.path.join(model_output_base_dir, f"{model_name}_{dataset_name}")
    os.makedirs(model_output_dir, exist_ok=True)
    
    # è®­ç»ƒå‚æ•°é…ç½®
    training_args = TrainingArguments(
        output_dir=model_output_dir,
        num_train_epochs=epochs,
        eval_strategy="epoch",  # æ–°ç‰ˆtransformersä½¿ç”¨eval_strategy
        save_strategy="epoch",
        learning_rate=learning_rate,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        weight_decay=0.01,
        load_best_model_at_end=True,
        save_total_limit=1,
        logging_dir=os.path.join(model_output_dir, 'logs'),
        logging_steps=100,
        metric_for_best_model='f1',
        greater_is_better=True,
        seed=seed,
    )
    
    # [FIX 4] å…¼å®¹ transformers v4.x å’Œ v5.x
    # v5.0+ ä½¿ç”¨ processing_class, v4.x ä½¿ç”¨ tokenizer
    trainer_kwargs = dict(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        compute_metrics=compute_metrics,
    )
    if int(transformers.__version__.split('.')[0]) >= 5:
        trainer_kwargs['processing_class'] = tokenizer
    else:
        trainer_kwargs['tokenizer'] = tokenizer

    trainer = Trainer(**trainer_kwargs)
    
    # è®­ç»ƒ
    print("Starting training...")
    trainer.train()
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    trainer.save_model(model_output_dir)
    tokenizer.save_pretrained(model_output_dir)
    print(f"Model saved to: {model_output_dir}")
    
    # åœæ­¢ç¢³æ’æ”¾è¿½è¸ª
    training_emissions = tracker.stop()
    print(f"Training emissions: {training_emissions:.6f} kg CO2")
    
    # ä¿å­˜ç¢³æ’æ”¾è®°å½•
    emissions_file = os.path.join(model_output_dir, 'training_emissions.txt')
    with open(emissions_file, 'w') as f:
        f.write(f"Model: {model_name}\n")
        f.write(f"Training emissions: {training_emissions:.6f} kg CO2\n")
    
    return model_output_dir, training_emissions


# ==================== æ¨¡å‹è¯„ä¼°å‡½æ•° ====================
def evaluate_model(test_data, model_output_dir, model_name, result_output_base_dir='result_output', 
                   dataset_name='cold', seed=42):
    """
    åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹
    
    Args:
        test_data: æµ‹è¯•æ•°æ®DataFrame
        model_output_dir: å·²è®­ç»ƒæ¨¡å‹çš„è·¯å¾„
        model_name: æ¨¡å‹åç§°
        result_output_base_dir: ç»“æœè¾“å‡ºåŸºç¡€ç›®å½•
        dataset_name: æ•°æ®é›†åç§°
        seed: éšæœºç§å­
    
    Returns:
        df_report: åˆ†ç±»æŠ¥å‘ŠDataFrame
        eval_emissions: è¯„ä¼°é˜¶æ®µç¢³æ’æ”¾é‡
    """
    print(f"\n{'='*60}")
    print(f"Evaluating model: {model_name}")
    print(f"Model path: {model_output_dir}")
    print(f"Test dataset: {dataset_name}")
    print(f"{'='*60}")
    
    np.random.seed(seed)
    num_labels = len(test_data['label'].unique())
    print(f"Number of unique labels: {num_labels}")
    print(f"Test data size: {len(test_data)}")
    
    # å¯åŠ¨ç¢³æ’æ”¾è¿½è¸ªï¼ˆè¯„ä¼°é˜¶æ®µä¹Ÿè¿½è¸ª - æ”¹è¿›åŸè®ºæ–‡ä»£ç ï¼‰
    tracker = EmissionsTracker()
    tracker.start()
    
    # [FIX 5] åŠ è½½è‡ªå·±è®­ç»ƒå¥½çš„æ¨¡å‹æ—¶ä¸åº”ä½¿ç”¨ ignore_mismatched_sizes
    print("Loading model and tokenizer...")
    model = AutoModelForSequenceClassification.from_pretrained(
        model_output_dir, 
        num_labels=num_labels,
    )
    tokenizer = AutoTokenizer.from_pretrained(model_output_dir)
    
    # åˆ›å»ºç»“æœè¾“å‡ºç›®å½•
    result_output_dir = os.path.join(result_output_base_dir, f"{model_name}_{dataset_name}")
    os.makedirs(result_output_dir, exist_ok=True)
    
    # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ï¼šä¼˜å…ˆGPUï¼ˆæ”¹è¿›åŸè®ºæ–‡ä»£ç ï¼‰
    device = 0 if torch.cuda.is_available() else -1
    device_name = "GPU" if device == 0 else "CPU"
    print(f"Using device: {device_name}")
    
    # åˆ›å»ºæ¨ç†pipeline
    pipe = pipeline(
        "text-classification", 
        model=model, 
        tokenizer=tokenizer, 
        device=device
    )
    
    # [FIX 6] æ¨ç†æ—¶å¿…é¡»åŠ  truncation + batch_sizeï¼Œå¦åˆ™é•¿æ–‡æœ¬ä¼šæŠ¥é”™/OOM
    print("Running predictions...")
    predictions = pipe(
        test_data['text'].tolist(), 
        top_k=None, 
        truncation=True, 
        max_length=512,
        batch_size=32,
    )
    
    # æå–é¢„æµ‹ç»“æœ (å…¼å®¹æ–°ç‰ˆæœ¬æ ¼å¼)
    def extract_prediction(pred):
        """ä»é¢„æµ‹ç»“æœä¸­æå–æ ‡ç­¾å’Œæ¦‚ç‡"""
        if isinstance(pred, list):
            # å¤šåˆ†ç±»ç»“æœ: [{'label': 'LABEL_0', 'score': 0.8}, {'label': 'LABEL_1', 'score': 0.2}]
            best = max(pred, key=lambda x: x['score'])
        else:
            # å•ä¸ªç»“æœ: {'label': 'LABEL_1', 'score': 0.9}
            best = pred
        
        label_str = best['label']
        # å¤„ç†ä¸åŒçš„æ ‡ç­¾æ ¼å¼: "LABEL_0", "LABEL_1" æˆ–ç›´æ¥ "0", "1"
        if 'LABEL_' in label_str:
            label = int(label_str.split('_')[-1])
        else:
            label = int(label_str)
        return label, best['score']
    
    pred_labels = []
    pred_probs = []
    for pred in predictions:
        label, prob = extract_prediction(pred)
        pred_labels.append(label)
        pred_probs.append(prob)
    
    y_true = test_data['label'].tolist()
    
    # åœæ­¢ç¢³æ’æ”¾è¿½è¸ª
    eval_emissions = tracker.stop()
    print(f"Evaluation emissions: {eval_emissions:.6f} kg CO2")
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    results_df = pd.DataFrame({
        'text': test_data['text'].tolist(),
        'predicted_label': pred_labels,
        'predicted_probability': pred_probs,
        'actual_label': y_true,
        'group': test_data['group'].tolist(),
        'dataset_name': test_data['data_name'].tolist()
    })
    
    results_file_path = os.path.join(result_output_dir, "full_results.csv")
    results_df.to_csv(results_file_path, index=False, encoding='utf-8-sig')
    print(f"Full results saved to: {results_file_path}")
    
    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    report = classification_report(y_true, pred_labels, output_dict=True)
    df_report = pd.DataFrame(report).transpose()
    
    report_file_path = os.path.join(result_output_dir, "classification_report.csv")
    df_report.to_csv(report_file_path, encoding='utf-8-sig')
    print(f"Classification report saved to: {report_file_path}")
    
    # æ‰“å°æŠ¥å‘Š
    print("\nClassification Report:")
    print(classification_report(y_true, pred_labels))
    
    # è®¡ç®—é¢å¤–æŒ‡æ ‡
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, pred_labels, average='macro')
    balanced_acc = balanced_accuracy_score(y_true, pred_labels)
    
    print(f"\nMacro Precision: {precision:.4f}")
    print(f"Macro Recall: {recall:.4f}")
    print(f"Macro F1: {f1:.4f}")
    print(f"Balanced Accuracy: {balanced_acc:.4f}")
    
    # ä¿å­˜ç¢³æ’æ”¾è®°å½•
    emissions_file = os.path.join(result_output_dir, 'evaluation_emissions.txt')
    with open(emissions_file, 'w') as f:
        f.write(f"Model: {model_name}\n")
        f.write(f"Evaluation emissions: {eval_emissions:.6f} kg CO2\n")
    
    return df_report, eval_emissions


# ==================== å®Œæ•´å®éªŒæµç¨‹å‡½æ•° ====================
def run_experiment(model_key, model_path, train_data, test_data, 
                   batch_size=32, epochs=6, learning_rate=2e-5, seed=42):
    """
    è¿è¡Œå•ä¸ªæ¨¡å‹çš„å®Œæ•´è®­ç»ƒå’Œè¯„ä¼°æµç¨‹
    
    Args:
        model_key: æ¨¡å‹é”®å
        model_path: æ¨¡å‹è·¯å¾„
        train_data: è®­ç»ƒæ•°æ®
        test_data: æµ‹è¯•æ•°æ®
        batch_size: æ‰¹æ¬¡å¤§å°
        epochs: è®­ç»ƒè½®æ•°
        learning_rate: å­¦ä¹ ç‡
        seed: éšæœºç§å­
    
    Returns:
        results: åŒ…å«æ¨¡å‹è¾“å‡ºè·¯å¾„å’Œè¯„ä¼°æŠ¥å‘Šçš„å­—å…¸
    """
    print(f"\n{'#'*70}")
    print(f"# Running experiment for: {model_key}")
    print(f"# Model: {model_path}")
    print(f"{'#'*70}")
    
    # è®­ç»ƒæ¨¡å‹
    model_output_dir, training_emissions = train_model(
        train_data=train_data,
        model_path=model_path,
        model_name=model_key,
        batch_size=batch_size,
        epochs=epochs,
        learning_rate=learning_rate,
        model_output_base_dir=f'model_output_{model_key}',
        dataset_name='cold',
        seed=seed
    )
    
    # è¯„ä¼°æ¨¡å‹
    report, eval_emissions = evaluate_model(
        test_data=test_data,
        model_output_dir=model_output_dir,
        model_name=model_key,
        result_output_base_dir=f'result_output_{model_key}',
        dataset_name='cold',
        seed=seed
    )
    
    return {
        'model_key': model_key,
        'model_path': model_path,
        'model_output_dir': model_output_dir,
        'report': report,
        'training_emissions': training_emissions,
        'eval_emissions': eval_emissions,
        'total_emissions': training_emissions + eval_emissions
    }


# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    
    # å®éªŒé…ç½®
    CONFIG = {
        'csv_file_path': BASE_DIR / 'cold.csv',  # æ•°æ®é›†è·¯å¾„
        'labelling_criteria': 'stereotype',       # æ­£ç±»æ ‡ç­¾
        'dataset_name': 'COLD',                   # æ•°æ®é›†åç§°
        'sample_size': 1000000,                   # é‡‡æ ·å¤§å°
        'batch_size': 32,                         # æ‰¹æ¬¡å¤§å°
        'epochs': 6,                              # è®­ç»ƒè½®æ•°
        'learning_rate': 2e-5,                    # å­¦ä¹ ç‡
        'seed': 42,                               # éšæœºç§å­
    }
    
    # é€‰æ‹©è¦ä½¿ç”¨çš„ä¸­æ–‡æ¨¡å‹ï¼ˆæŒ‰å‚æ•°é‡ä»å°åˆ°å¤§æ’åºï¼‰
    # é¡ºåº: albert(10M) -> rbt6(59M) -> macbert(102M) -> bert(102M)
    SELECTED_MODELS = OrderedDict([
    ('albert_chinese', CHINESE_MODELS['albert']),
    ('rbt6', CHINESE_MODELS['rbt6']),              # æ›¿ä»£ distilbert
    ('macbert', CHINESE_MODELS['macbert']),
    ('bert_chinese', CHINESE_MODELS['bert']),
    ])
    
    print("="*70)
    print("Chinese BERT Models Fine-Tuning for Stereotype Detection")
    print("Based on HEARTS Framework (NeurIPS 2024)")
    print("="*70)
    print(f"\nConfiguration:")
    for key, value in CONFIG.items():
        print(f"  {key}: {value}")
    print(f"\nSelected models (æŒ‰å‚æ•°é‡ä»å°åˆ°å¤§æ’åº):")
    print(f"  {'Model Key':<20} {'HuggingFace Path':<45} {'å‚æ•°é‡':<12} {'å¯¹åº”åŸè®ºæ–‡'}")
    print(f"  {'-'*20} {'-'*45} {'-'*12} {'-'*15}")
    print(f"  {'albert_chinese':<20} {CHINESE_MODELS['albert']:<45} {'~10M':<12} ALBERT-V2")
    print(f"  {'macbert':<20} {CHINESE_MODELS['macbert']:<45} {'~102M':<12} é¢å¤–å¯¹æ¯”")
    print(f"  {'bert_chinese':<20} {CHINESE_MODELS['bert']:<45} {'~102M':<12} BERT")
    
    # æ£€æµ‹è®¾å¤‡
    if torch.cuda.is_available():
        print(f"\nğŸš€ GPU detected: {torch.cuda.get_device_name(0)}")
    else:
        print(f"\nâš ï¸  No GPU detected, using CPU (training will be slower)")
    
    # åŠ è½½æ•°æ®
    train_data, test_data = data_loader(
        csv_file_path=CONFIG['csv_file_path'],
        labelling_criteria=CONFIG['labelling_criteria'],
        dataset_name=CONFIG['dataset_name'],
        sample_size=CONFIG['sample_size'],
        num_examples=5
    )
    
    # å­˜å‚¨æ‰€æœ‰å®éªŒç»“æœ
    all_results = []
    
    # å¯¹æ¯ä¸ªæ¨¡å‹è¿è¡Œå®éªŒ
    for model_key, model_path in SELECTED_MODELS.items():
        try:
            result = run_experiment(
                model_key=model_key,
                model_path=model_path,
                train_data=train_data,
                test_data=test_data,
                batch_size=CONFIG['batch_size'],
                epochs=CONFIG['epochs'],
                learning_rate=CONFIG['learning_rate'],
                seed=CONFIG['seed']
            )
            all_results.append(result)
        except Exception as e:
            print(f"\nâŒ Error running experiment for {model_key}: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*70)
    print("EXPERIMENT SUMMARY")
    print("="*70)
    
    summary_data = []
    for result in all_results:
        report = result['report']
        if 'macro avg' in report.index:
            macro_f1 = report.loc['macro avg', 'f1-score']
            macro_precision = report.loc['macro avg', 'precision']
            macro_recall = report.loc['macro avg', 'recall']
        else:
            macro_f1 = macro_precision = macro_recall = None
            
        summary_data.append({
            'Model': result['model_key'],
            'Model Path': result['model_path'],
            'Macro Precision': macro_precision,
            'Macro Recall': macro_recall,
            'Macro F1': macro_f1,
            'Training Emissions (kg CO2)': result['training_emissions'],
            'Eval Emissions (kg CO2)': result['eval_emissions'],
            'Total Emissions (kg CO2)': result['total_emissions']
        })
        
        print(f"\n{result['model_key']}:")
        print(f"  Model Path: {result['model_path']}")
        print(f"  Output Dir: {result['model_output_dir']}")
        if macro_f1:
            print(f"  Macro Precision: {macro_precision:.4f}")
            print(f"  Macro Recall: {macro_recall:.4f}")
            print(f"  Macro F1: {macro_f1:.4f}")
        print(f"  Training Emissions: {result['training_emissions']:.6f} kg CO2")
        print(f"  Eval Emissions: {result['eval_emissions']:.6f} kg CO2")
        print(f"  Total Emissions: {result['total_emissions']:.6f} kg CO2")
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    summary_df = pd.DataFrame(summary_data)
    summary_df.to_csv('experiment_summary.csv', index=False, encoding='utf-8-sig')
    print(f"\nğŸ“Š Summary saved to: experiment_summary.csv")
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print("\n" + "="*90)
    print("MODEL COMPARISON TABLE (å¯¹æ ‡åŸè®ºæ–‡ Table 1)")
    print("="*90)
    print(f"\n{'Model':<18} {'Precision':<12} {'Recall':<12} {'Macro F1':<12} {'Emissions (kg CO2)':<20}")
    print(f"{'-'*18} {'-'*12} {'-'*12} {'-'*12} {'-'*20}")
    for data in summary_data:
        p = f"{data['Macro Precision']:.4f}" if data['Macro Precision'] else "N/A"
        r = f"{data['Macro Recall']:.4f}" if data['Macro Recall'] else "N/A"
        f1 = f"{data['Macro F1']:.4f}" if data['Macro F1'] else "N/A"
        emissions = f"{data['Total Emissions (kg CO2)']:.6f}"
        print(f"{data['Model']:<18} {p:<12} {r:<12} {f1:<12} {emissions:<20}")
    
    print("\n" + "="*70)
    print("âœ… All experiments completed!")
    print("="*70)
    
    # æ‰“å°æ”¹è¿›è¯´æ˜
    print("\nğŸ“ ç›¸æ¯”åŸè®ºæ–‡ä»£ç çš„æ”¹è¿›:")
    print("  1. è¯„ä¼°é˜¶æ®µä¹Ÿè¿½è¸ªç¢³æ’æ”¾ï¼ˆåŸä»£ç æœªè¿½è¸ªï¼‰")
    print("  2. è¯„ä¼°é˜¶æ®µä¼˜å…ˆä½¿ç”¨GPUï¼ˆåŸä»£ç å¼ºåˆ¶ç”¨CPUï¼‰")
    print("  3. è®°å½•å®Œæ•´çš„ç¢³æ’æ”¾æ•°æ®ï¼ˆè®­ç»ƒ+è¯„ä¼°ï¼‰")