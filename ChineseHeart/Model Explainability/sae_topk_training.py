# SAE Training with TopK Sparsity
# ä½¿ç”¨ TopK å¼ºåˆ¶ç¨€ç–çš„ SAE è®­ç»ƒ
# è§£å†³ L1 æƒ©ç½šæ— æ•ˆçš„é—®é¢˜

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
import pandas as pd
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from tqdm.auto import tqdm
import os
import json


# ==================== é…ç½® ====================
class SAEConfig:
    # æ¨¡å‹è·¯å¾„
    model_dir = "model_output_albert_chinese/albert_chinese_cold"
    data_path = "cold.csv"
    layer_idx = 8
    
    # SAE å‚æ•°
    hidden_dim = 768
    expansion_factor = 8
    
    @property
    def sae_dim(self):
        return self.hidden_dim * self.expansion_factor
    
    # â­ TopK ç¨€ç–å‚æ•°
    # K å€¼å†³å®šæ¯ä¸ªæ ·æœ¬æœ€å¤šæ¿€æ´»å¤šå°‘ä¸ªç‰¹å¾
    # æ¨è: sae_dim çš„ 1-5%
    k = 64  # æ¯ä¸ªæ ·æœ¬åªæ¿€æ´» 64 ä¸ªç‰¹å¾ (64/6144 â‰ˆ 1%)
    
    # è®­ç»ƒå‚æ•°
    batch_size = 32
    learning_rate = 3e-4
    num_epochs = 10
    
    # æ•°æ®å‚æ•°
    max_length = 256
    num_samples = None
    output_dir = "sae_output_topk"
    
    @property
    def device(self):
        return "cuda" if torch.cuda.is_available() else "cpu"


# ==================== TopK SAE æ¨¡å‹ ====================
class TopKSparseAutoencoder(nn.Module):
    """
    TopK ç¨€ç–è‡ªç¼–ç å™¨
    å¼ºåˆ¶æ¯ä¸ªæ ·æœ¬åªæœ‰ K ä¸ªç‰¹å¾æ¿€æ´»
    """
    
    def __init__(self, input_dim, sae_dim, k):
        super().__init__()
        self.input_dim = input_dim
        self.sae_dim = sae_dim
        self.k = k
        
        # ç¼–ç å™¨
        self.encoder = nn.Linear(input_dim, sae_dim)
        
        # è§£ç å™¨ï¼ˆæƒé‡å¯ä»¥ä¸ç¼–ç å™¨ç»‘å®šæˆ–ç‹¬ç«‹ï¼‰
        self.decoder = nn.Linear(sae_dim, input_dim)
        
        self._init_weights()
    
    def _init_weights(self):
        # ä½¿ç”¨è¾ƒå°çš„åˆå§‹åŒ–ï¼Œæœ‰åŠ©äºç¨€ç–æ€§
        nn.init.xavier_uniform_(self.encoder.weight)
        nn.init.zeros_(self.encoder.bias)
        nn.init.xavier_uniform_(self.decoder.weight)
        nn.init.zeros_(self.decoder.bias)
    
    def encode(self, x):
        """
        ç¼–ç  + TopK ç¨€ç–åŒ–
        """
        # çº¿æ€§å˜æ¢
        pre_act = self.encoder(x)  # [batch, sae_dim]
        
        # ReLU æ¿€æ´»
        pre_act = F.relu(pre_act)
        
        # TopK ç¨€ç–åŒ–ï¼šåªä¿ç•™æœ€å¤§çš„ K ä¸ªå€¼
        topk_values, topk_indices = torch.topk(pre_act, self.k, dim=-1)
        
        # åˆ›å»ºç¨€ç–æ¿€æ´»
        sparse_act = torch.zeros_like(pre_act)
        sparse_act.scatter_(-1, topk_indices, topk_values)
        
        return sparse_act
    
    def decode(self, h):
        return self.decoder(h)
    
    def forward(self, x):
        h = self.encode(x)
        recon = self.decode(h)
        return recon, h


# ==================== æ¿€æ´»æå– ====================
def extract_activations(model, tokenizer, texts, layer_idx, config):
    """ä»æ¨¡å‹æå–æ¿€æ´»"""
    model.eval()
    all_activations = []
    
    with torch.no_grad():
        for i in tqdm(range(0, len(texts), config.batch_size), desc="æå–æ¿€æ´»"):
            batch = texts[i:i+config.batch_size]
            
            inputs = tokenizer(
                batch, padding=True, truncation=True,
                max_length=config.max_length, return_tensors="pt"
            ).to(config.device)
            
            outputs = model(**inputs, output_hidden_states=True)
            hidden = outputs.hidden_states[layer_idx + 1][:, 0, :].cpu()
            all_activations.append(hidden)
    
    return torch.cat(all_activations, dim=0)


# ==================== è®­ç»ƒå‡½æ•° ====================
def train_topk_sae(activations, config):
    """è®­ç»ƒ TopK SAE"""
    
    dataset = torch.utils.data.TensorDataset(activations)
    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)
    
    # åˆ›å»º TopK SAE
    sae = TopKSparseAutoencoder(
        config.hidden_dim, 
        config.sae_dim, 
        config.k
    ).to(config.device)
    
    optimizer = torch.optim.Adam(sae.parameters(), lr=config.learning_rate)
    
    history = {'loss': [], 'l0': []}
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ Training TopK SAE")
    print(f"   Device: {config.device}")
    print(f"   Dimensions: {config.hidden_dim} -> {config.sae_dim}")
    print(f"   K (max active features): {config.k}")
    print(f"   Target sparsity: {config.k}/{config.sae_dim} = {100*config.k/config.sae_dim:.1f}%")
    print(f"{'='*60}\n")
    
    for epoch in range(config.num_epochs):
        total_loss = 0
        total_l0 = 0
        
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{config.num_epochs}", leave=False)
        for (batch,) in pbar:
            batch = batch.to(config.device)
            
            recon, h = sae(batch)
            
            # åªç”¨é‡å»ºæŸå¤±ï¼ˆTopK å·²ç»ä¿è¯ç¨€ç–æ€§ï¼‰
            loss = F.mse_loss(recon, batch)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            total_l0 += (h > 0).float().sum(-1).mean().item()
            
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        n = len(dataloader)
        avg_loss = total_loss / n
        avg_l0 = total_l0 / n
        
        history['loss'].append(avg_loss)
        history['l0'].append(avg_l0)
        
        print(f"Epoch {epoch+1}/{config.num_epochs}: "
              f"Loss={avg_loss:.6f}, L0={avg_l0:.1f}/{config.sae_dim} "
              f"({100*avg_l0/config.sae_dim:.1f}%)")
    
    return sae, history


# ==================== ç‰¹å¾åˆ†æ ====================
def analyze_features(sae, activations, texts, labels, config, top_k=10):
    """åˆ†æå­¦åˆ°çš„ç‰¹å¾"""
    
    sae.eval()
    with torch.no_grad():
        h = sae.encode(activations.to(config.device)).cpu()
    
    labels_t = torch.tensor(labels)
    
    # è®¡ç®—ç‰¹å¾åœ¨æ­£è´Ÿæ ·æœ¬ä¸Šçš„æ¿€æ´»å·®å¼‚
    pos_mean = h[labels_t == 1].mean(0)
    neg_mean = h[labels_t == 0].mean(0)
    diff = pos_mean - neg_mean
    
    # æœ€ç›¸å…³ç‰¹å¾
    top_pos = torch.topk(diff, top_k).indices.tolist()
    top_neg = torch.topk(-diff, top_k).indices.tolist()
    
    print("\n" + "="*60)
    print("ğŸ“Š ä¸ã€åˆ»æ¿å°è±¡ã€‘æœ€ç›¸å…³çš„ç‰¹å¾ (æ­£å‘)")
    print("="*60)
    
    for idx in top_pos[:5]:
        feat_act = h[:, idx]
        top_samples = torch.topk(feat_act, 5).indices.tolist()
        print(f"\nğŸ”¸ Feature #{idx} (diff: {diff[idx]:.4f})")
        for i, s_idx in enumerate(top_samples[:3]):
            print(f"   [{feat_act[s_idx]:.3f}] {texts[s_idx][:50]}...")
    
    print("\n" + "="*60)
    print("ğŸ“Š ä¸ã€éåˆ»æ¿å°è±¡ã€‘æœ€ç›¸å…³çš„ç‰¹å¾ (è´Ÿå‘)")
    print("="*60)
    
    for idx in top_neg[:5]:
        feat_act = h[:, idx]
        top_samples = torch.topk(feat_act, 5).indices.tolist()
        print(f"\nğŸ”¹ Feature #{idx} (diff: {diff[idx]:.4f})")
        for i, s_idx in enumerate(top_samples[:3]):
            print(f"   [{feat_act[s_idx]:.3f}] {texts[s_idx][:50]}...")
    
    return {'pos_features': top_pos, 'neg_features': top_neg, 'diff_scores': diff}


# ==================== ä¸»å‡½æ•° ====================
def main():
    config = SAEConfig()
    
    print("="*70)
    print("ğŸ§  TopK SAE Training for Chinese ALBERT")
    print("="*70)
    
    # è®¾å¤‡ä¿¡æ¯
    print(f"\nğŸ“± Device: {config.device}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ“¦ Loading model: {config.model_dir}")
    model = AutoModelForSequenceClassification.from_pretrained(config.model_dir)
    tokenizer = AutoTokenizer.from_pretrained(config.model_dir)
    model = model.to(config.device)
    print("   âœ… Model loaded")
    
    # åŠ è½½æ•°æ®
    print(f"\nğŸ“‚ Loading data: {config.data_path}")
    df = pd.read_csv(config.data_path, usecols=['text', 'label'])
    label_map = {'stereotype': 1, 'non-stereotype': 0, 'neutral': 0, 'unrelated': 0}
    df['label_binary'] = df['label'].map(lambda x: label_map.get(x, 0))
    
    if config.num_samples:
        df = df.sample(n=config.num_samples, random_state=42)
    
    texts = df['text'].tolist()
    labels = df['label_binary'].tolist()
    print(f"   âœ… Loaded {len(texts)} samples")
    print(f"   Label distribution: {pd.Series(labels).value_counts().to_dict()}")
    
    # æå–æ¿€æ´»
    print(f"\nğŸ” Extracting layer {config.layer_idx} activations...")
    activations = extract_activations(model, tokenizer, texts, config.layer_idx, config)
    print(f"   âœ… Shape: {activations.shape}")
    
    # è®­ç»ƒ TopK SAE
    print(f"\nğŸ‹ï¸ Training TopK SAE (K={config.k})...")
    sae, history = train_topk_sae(activations, config)
    
    # ä¿å­˜
    os.makedirs(config.output_dir, exist_ok=True)
    torch.save(sae.state_dict(), f"{config.output_dir}/sae_topk_weights.pt")
    
    save_config = {
        'hidden_dim': config.hidden_dim,
        'sae_dim': config.sae_dim,
        'k': config.k,
        'layer_idx': config.layer_idx,
    }
    with open(f"{config.output_dir}/config.json", 'w') as f:
        json.dump(save_config, f, indent=2)
    
    with open(f"{config.output_dir}/history.json", 'w') as f:
        json.dump(history, f, indent=2)
    
    print(f"\nğŸ’¾ Saved to: {config.output_dir}/")
    
    # åˆ†æç‰¹å¾
    print(f"\nğŸ”¬ Analyzing features...")
    analysis = analyze_features(sae, activations, texts, labels, config)
    
    # ä¿å­˜åˆ†æ
    analysis_save = {
        'pos_features': analysis['pos_features'],
        'neg_features': analysis['neg_features'],
    }
    with open(f"{config.output_dir}/feature_analysis.json", 'w') as f:
        json.dump(analysis_save, f, indent=2)
    
    print("\n" + "="*70)
    print("âœ… TopK SAE Training Complete!")
    print("="*70)
    print(f"\nğŸ“Š Summary:")
    print(f"   - Final Loss: {history['loss'][-1]:.6f}")
    print(f"   - L0: {history['l0'][-1]:.1f}/{config.sae_dim} ({100*history['l0'][-1]/config.sae_dim:.1f}%)")
    print(f"   - K (target): {config.k} ({100*config.k/config.sae_dim:.1f}%)")


if __name__ == "__main__":
    main()
